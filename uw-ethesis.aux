\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{ix}{section*.2}}
\citation{martens2010deep}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xi}{section*.4}}
\citation{Shepherd.1997}
\citation{bojarski2016end}
\citation{krizhevsky2012imagenet}
\citation{ma2002natural}
\citation{silver2016mastering}
\citation{hornik1989multilayer}
\citation{hastie2009overview}
\citation{priddy2005artificial}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{neural1}
\citation{neural2}
\citation{gulcehre2017robust}
\citation{simpson2015oddball}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{martens2010deep}
\citation{wright1999numerical}
\citation{shultz1985family}
\citation{Pearlmutter.1993}
\citation{adachi.paper}
\citation{adachi.paper}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Summary of Contributions}{4}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Outline}{4}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Training Feed Forward Artificial Neural Networks}{6}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Feed Forward Artificial Neural Networks}{6}{section.2.1}}
\newlabel{equation:training_example}{{2.1}{6}{Feed Forward Artificial Neural Networks}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Perceptron}{7}{subsection.2.1.1}}
\newlabel{equation:perceptron_output}{{2.2}{7}{Perceptron}{equation.2.1.2}{}}
\newlabel{equation:heaviside_step}{{2.3}{7}{Perceptron}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Multiple Layers for Complex Models}{7}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphical representation of an example perceptron which predicts the output for the $k^{th}$ sample, $z^{k}$ with $\mathbf  {x}^{k} \in R^{3}$. The ``1" is a constant multiplier which differentiates regular weights values from the bias value $b$.\relax }}{8}{figure.caption.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{perceptron}{{2.1}{8}{Graphical representation of an example perceptron which predicts the output for the $k^{th}$ sample, $z^{k}$ with $\mathbf {x}^{k} \in R^{3}$. The ``1" is a constant multiplier which differentiates regular weights values from the bias value $b$.\relax }{figure.caption.12}{}}
\newlabel{equation:classification_function}{{2.6}{8}{Multiple Layers for Complex Models}{equation.2.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the capacity increase from having multiple layers in a network. Subplot a) shows the classification potential for a single perceptron. Subplots b) and c) show the potential classification performed by 3 perceptrons, 2 in the first layer and 1 in the second.\relax }}{9}{figure.caption.16}}
\newlabel{figure:MLP}{{2.2}{9}{Visualization of the capacity increase from having multiple layers in a network. Subplot a) shows the classification potential for a single perceptron. Subplots b) and c) show the potential classification performed by 3 perceptrons, 2 in the first layer and 1 in the second.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Training}{9}{subsection.2.1.3}}
\newlabel{equation:objective_function}{{2.9}{9}{Training}{equation.2.1.9}{}}
\citation{dropout}
\newlabel{equation:optimization}{{2.10}{10}{Training}{equation.2.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Overfitting}{10}{subsection.2.1.4}}
\newlabel{equation:regularization}{{2.11}{10}{Overfitting}{equation.2.1.11}{}}
\citation{Shepherd.1997}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Optimization Methods for Training ff-ANNs}{11}{section.2.2}}
\newlabel{equation:optimization_simple}{{2.12}{11}{Optimization Methods for Training ff-ANNs}{equation.2.2.12}{}}
\citation{gd_converges.paper}
\citation{Shepherd.1997}
\citation{martens2010deep}
\citation{martens2010deep}
\citation{martens2010deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}First Order Methods}{12}{subsection.2.2.1}}
\newlabel{equation:gd_update}{{2.13}{12}{First Order Methods}{equation.2.2.13}{}}
\newlabel{equation:sgd_update}{{2.14}{12}{First Order Methods}{equation.2.2.14}{}}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces This is Figure 1 from \cite  {martens2010deep} which shows an example of a valley characteristic. On the left the arrows point towards the gradient direction and the red arrow points towards the shortest path to the smallest value visible. On the right are arrows pointing in the direction of a method considering curvature and therefore avoiding the gradient direction which is, in this case, in a direction of high positive curvature.\relax }}{13}{figure.caption.23}}
\newlabel{figure:valley}{{2.3}{13}{This is Figure 1 from \cite {martens2010deep} which shows an example of a valley characteristic. On the left the arrows point towards the gradient direction and the red arrow points towards the shortest path to the smallest value visible. On the right are arrows pointing in the direction of a method considering curvature and therefore avoiding the gradient direction which is, in this case, in a direction of high positive curvature.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Second Order Methods}{13}{subsection.2.2.2}}
\newlabel{section:second_order}{{2.2.2}{13}{Second Order Methods}{subsection.2.2.2}{}}
\newlabel{equation:newtons}{{2.15}{13}{Second Order Methods}{equation.2.2.15}{}}
\citation{Shepherd.1997}
\citation{saddle_free}
\citation{TRM.book}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Trust Region Methods}{14}{section.2.3}}
\citation{TRM.book}
\newlabel{model def}{{1}{15}{Trust Region Methods}{Item.17}{}}
\newlabel{step calc}{{2}{15}{Trust Region Methods}{Item.18}{}}
\newlabel{accept step}{{3}{15}{Trust Region Methods}{Item.19}{}}
\newlabel{update radius}{{4}{15}{Trust Region Methods}{Item.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Subproblem Definition}{15}{subsection.2.3.1}}
\newlabel{equation:taylor_series}{{2.17}{15}{Subproblem Definition}{equation.2.3.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Step Calculation: Solving the Trust Region Subproblem}{15}{subsection.2.3.2}}
\newlabel{equation:TRS}{{2.18}{15}{Step Calculation: Solving the Trust Region Subproblem}{equation.2.3.18}{}}
\citation{adachi.paper}
\citation{boyd}
\citation{adachi.paper}
\newlabel{equation:delta}{{2.19}{16}{Step Calculation: Solving the Trust Region Subproblem}{equation.2.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimality Conditions}{16}{section*.29}}
\newlabel{equation:inbounds}{{2.20}{16}{Optimality Conditions}{equation.2.3.20}{}}
\newlabel{equation:lagrangian}{{2.21}{16}{Optimality Conditions}{equation.2.3.21}{}}
\newlabel{equation:complementary_slackness}{{2.22}{16}{Optimality Conditions}{equation.2.3.22}{}}
\newlabel{equation:dual}{{2.23}{16}{Optimality Conditions}{equation.2.3.23}{}}
\citation{adachi.paper}
\newlabel{equation:grad_lagrangian}{{2.24}{17}{Optimality Conditions}{equation.2.3.24}{}}
\newlabel{equation:infimum}{{2.25}{17}{Optimality Conditions}{equation.2.3.25}{}}
\newlabel{equation:dual2}{{2.26}{17}{Optimality Conditions}{equation.2.3.26}{}}
\newlabel{equation:possemdef}{{2.27}{17}{Optimality Conditions}{equation.2.3.27}{}}
\newlabel{equation:grad_d}{{2.28}{17}{Optimality Conditions}{equation.2.3.28}{}}
\citation{TRM.book}
\citation{adachi.paper}
\citation{TRM.book}
\citation{adachi.paper}
\citation{adachi.paper}
\newlabel{equation:grad_lagrangian2}{{2.30}{18}{Optimality Conditions}{equation.2.3.30}{}}
\newlabel{phi}{{2.31}{18}{Optimality Conditions}{equation.2.3.31}{}}
\citation{adachi.paper}
\citation{adachi.paper}
\@writefile{toc}{\contentsline {subsubsection}{Solving TRS via a Generalized Eigenvalue Problem}{19}{section*.42}}
\@writefile{toc}{\contentsline {subsubsection}{Interior Case}{19}{section*.43}}
\newlabel{equation:p0}{{2.32a}{19}{Interior Case}{equation.2.3.32a}{}}
\newlabel{equation:p0_posdef}{{2.32b}{19}{Interior Case}{equation.2.3.32b}{}}
\newlabel{equation:p0_bounds}{{2.32c}{19}{Interior Case}{equation.2.3.32c}{}}
\citation{adachi.paper}
\citation{adachi.paper}
\@writefile{toc}{\contentsline {subsubsection}{Boundary Case via a Generalized Eigenvalue Problem}{20}{section*.45}}
\newlabel{equation:boundary}{{2.33}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.33}{}}
\newlabel{equation:M_def}{{2.35}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.35}{}}
\newlabel{equation:M}{{2.36}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.36}{}}
\newlabel{equation:p1}{{2.37}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.37}{}}
\newlabel{equation:M_def1}{{2.38}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.38}{}}
\newlabel{equation:M_def2}{{2.39}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.39}{}}
\newlabel{equation:M_def2_rearranged}{{2.40}{20}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.40}{}}
\citation{adachi.paper}
\citation{adachi.paper}
\newlabel{equation:v1v2}{{2.41}{21}{Boundary Case via a Generalized Eigenvalue Problem}{equation.2.3.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{Choosing the trial point}{21}{section*.55}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Selecting Global Minimizer, $\mathbf  {p}$, to (\ref  {equation:TRS})\relax }}{21}{algorithm.1}}
\newlabel{algorithm:trialpoint}{{1}{21}{Selecting Global Minimizer, $\mathbf {p}$, to (\ref {equation:TRS})\relax }{algorithm.1}{}}
\citation{Fletcher.1987}
\citation{Fletcher.1987}
\citation{Fletcher.1987}
\citation{Fletcher.1987}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Acceptance of the trial point}{22}{subsection.2.3.3}}
\newlabel{equation:rho}{{2.42}{22}{Acceptance of the trial point}{equation.2.3.42}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Acceptance of the trial point \cite  {Fletcher.1987}\relax }}{22}{algorithm.2}}
\newlabel{algorithm:accept_trial_point}{{2}{22}{Acceptance of the trial point \cite {Fletcher.1987}\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Trust region radius update}{22}{subsection.2.3.4}}
\newlabel{equation:lbub}{{2.43}{22}{Trust region radius update}{equation.2.3.43}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adaptive adjustment of trust region size\relax }}{23}{algorithm.3}}
\newlabel{trust region size}{{3}{23}{Adaptive adjustment of trust region size\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Method Summary}{23}{subsection.2.3.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Full Trust Region Algorithm to Solve (\ref  {equation:optimization_simple})\relax }}{24}{algorithm.4}}
\newlabel{trust region method}{{4}{24}{Full Trust Region Algorithm to Solve (\ref {equation:optimization_simple})\relax }{algorithm.4}{}}
\citation{adachi.paper}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Solving the Trust Region Subproblem for Feedforward Neural Networks}{25}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Computing the Solution to the TRS}{25}{section.3.1}}
\citation{demmel.book}
\citation{demmel.book}
\citation{IRAM}
\citation{Pearlmutter.1993}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Computing Second Order Derivatives}{26}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Conjugate Gradient}{26}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Implicitly Restarted Arnoldi Method}{26}{subsection.3.1.3}}
\citation{Pearlmutter.1993}
\citation{Pearlmutter.1993}
\citation{adachi.paper}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Conjugate Gradient, Algorithm 6.11 from \cite  {demmel.book}\relax }}{27}{algorithm.5}}
\newlabel{algorithm:cg}{{5}{27}{Conjugate Gradient, Algorithm 6.11 from \cite {demmel.book}\relax }{algorithm.5}{}}
\newlabel{algorithm:cg}{{14}{27}{Conjugate Gradient, Algorithm 6.11 from \cite {demmel.book}\relax }{algorithm.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Computing $\mathbf  {H}_{k}$}{27}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Computing The Boundary Solution, $\mathbf  {p}_{1}$}{27}{subsection.3.1.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Compute $\mathbf  {H}_{k}$, using the Pearlmutter Trick \cite  {Pearlmutter.1993}\relax }}{28}{algorithm.6}}
\newlabel{algorithm:hv}{{6}{28}{Compute $\mathbf {H}_{k}$, using the Pearlmutter Trick \cite {Pearlmutter.1993}\relax }{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Compute $\mathbf  {p}_{1}$\relax }}{28}{algorithm.7}}
\newlabel{algorithm:p1}{{7}{28}{Compute $\mathbf {p}_{1}$\relax }{algorithm.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Summary of TRS Solution Method}{29}{subsection.3.1.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Solving the TRS\relax }}{29}{algorithm.8}}
\newlabel{algorithm:trs}{{8}{29}{Solving the TRS\relax }{algorithm.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Modified Trust Region Methods}{30}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Challenge: Stochastic Variations on the \textbf  {TRM}}{30}{subsection.4.0.1}}
\citation{mnist}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Stochastic Subsampling of Training Samples}{31}{section.4.1}}
\citation{tutorial}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training an ff-ANN to predict MNIST using \textbf  {SGD}, \textbf  {MBGD} and \textbf  {GD} for comparison. \relax }}{32}{figure.caption.63}}
\newlabel{figure:SMBGDGD}{{4.1}{32}{Training an ff-ANN to predict MNIST using \textbf {SGD}, \textbf {MBGD} and \textbf {GD} for comparison. \relax }{figure.caption.63}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Adaptive Trust Region Reduction Schedule (ATRRS)\relax }}{33}{algorithm.9}}
\newlabel{algorithm:assrs-simple}{{9}{33}{Adaptive Trust Region Reduction Schedule (ATRRS)\relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Weight Subsampling}{34}{section.4.2}}
\newlabel{equation:w_subset}{{4.9}{34}{Weight Subsampling}{equation.4.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hybrid Approach}{35}{section.4.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces \textbf  {MBGD} $\eta $ Update and \textbf  {TRM} Step Incorporation\relax }}{35}{algorithm.10}}
\newlabel{algorithm:TRMMBGD}{{10}{35}{\textbf {MBGD} $\eta $ Update and \textbf {TRM} Step Incorporation\relax }{algorithm.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Full Hessian vs Hessian Free}{35}{section.4.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The approach used for computing Hessian information in each method. HF refers to ``Hessian Free" and FH refers to ``Full Hessian". HF is in bold to more easily distinguish between the two values. \relax }}{36}{table.caption.70}}
\newlabel{table:FHHF}{{4.1}{36}{The approach used for computing Hessian information in each method. HF refers to ``Hessian Free" and FH refers to ``Full Hessian". HF is in bold to more easily distinguish between the two values. \relax }{table.caption.70}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Summary}{36}{section.4.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Training algorithm definitions used for numerical exploration, and their labels, which will be used to reference them.\relax }}{37}{table.caption.71}}
\newlabel{table:algorithms}{{4.2}{37}{Training algorithm definitions used for numerical exploration, and their labels, which will be used to reference them.\relax }{table.caption.71}{}}
\citation{mnist}
\citation{Derm}
\citation{IRIS}
\citation{Nurs}
\citation{Habe}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Numerical Results}{38}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Experimental Set-up}{38}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Datasets}{38}{subsection.5.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Datasets used for experimentation. *Habe uses a single dependent variable with two states, each to represent one of the classes.\relax }}{39}{table.caption.72}}
\newlabel{table:datasets}{{5.1}{39}{Datasets used for experimentation. *Habe uses a single dependent variable with two states, each to represent one of the classes.\relax }{table.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Datasets used for experimentation.\relax }}{39}{table.caption.73}}
\newlabel{table:datasettopics}{{5.2}{39}{Datasets used for experimentation.\relax }{table.caption.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Network Structure}{39}{subsection.5.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Visualization of Habe dataset where classes are represented by shape and colour. This shows the difficulty in learning this dataset, there is no clear separation between classes.\relax }}{40}{figure.caption.74}}
\newlabel{figure:haberman}{{5.1}{40}{Visualization of Habe dataset where classes are represented by shape and colour. This shows the difficulty in learning this dataset, there is no clear separation between classes.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Structure of ff-ANN for learning the Derm dataset. Used as visual example to show resulting structure for a given $n_{0}$ and $n_{2}$ which are based on the dataset.\relax }}{41}{figure.caption.76}}
\newlabel{figure:structure}{{5.2}{41}{Structure of ff-ANN for learning the Derm dataset. Used as visual example to show resulting structure for a given $n_{0}$ and $n_{2}$ which are based on the dataset.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Methods for Comparison}{42}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Time Required for a Reasonable Solution}{42}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}CPU time of TRM vs SGD}{43}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Time taken to reach within 1\% of minimum $f(\mathbf  {w})$ achieved by \textbf  {SGD} for both \textbf  {SGD} and \textbf  {TRM}.\relax }}{44}{table.caption.81}}
\newlabel{t_trm_sgd}{{5.3}{44}{Time taken to reach within 1\% of minimum $f(\mathbf {w})$ achieved by \textbf {SGD} for both \textbf {SGD} and \textbf {TRM}.\relax }{table.caption.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Using TRM and MBGD in Hybrid: TRMMBGD}{44}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Running TRMMBGD vs MBGD for training ff-ANNs on five datasets.\relax }}{45}{figure.caption.82}}
\newlabel{figure:TRMMBGD}{{5.3}{45}{Running TRMMBGD vs MBGD for training ff-ANNs on five datasets.\relax }{figure.caption.82}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces TRRS results for $\Delta = 1 \%$\relax }}{46}{table.caption.83}}
\newlabel{TRMMBGD1}{{5.4}{46}{TRRS results for $\Delta = 1 \%$\relax }{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces TRRS results for $\Delta = 5 \%$\relax }}{46}{table.caption.84}}
\newlabel{TRMMBGD5}{{5.5}{46}{TRRS results for $\Delta = 5 \%$\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces TRRS results for $\Delta = 10 \%$\relax }}{46}{table.caption.85}}
\newlabel{TRMMBGD10}{{5.6}{46}{TRRS results for $\Delta = 10 \%$\relax }{table.caption.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Stochastic TRMs}{47}{section.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Adaptive Trust Region Reduction Scheme}{47}{subsection.5.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparing \textbf  {TRM} with stochastic training example subsampling \textbf  {TRM} methods (\textbf  {STRM} and \textbf  {MBTRM}), based on objective function, $f(\mathbf  {w})$, vs CPU time.\relax }}{48}{figure.caption.86}}
\newlabel{figure:SMBTRMTRM}{{5.4}{48}{Comparing \textbf {TRM} with stochastic training example subsampling \textbf {TRM} methods (\textbf {STRM} and \textbf {MBTRM}), based on objective function, $f(\mathbf {w})$, vs CPU time.\relax }{figure.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces TRRS results for $\Delta = 1 \%$\relax }}{49}{table.caption.87}}
\newlabel{SMBTRMTRM1}{{5.7}{49}{TRRS results for $\Delta = 1 \%$\relax }{table.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Traditional TRM $\gamma $ Update Scheme}{49}{subsection.5.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces TRRS results for $\Delta = 5 \%$\relax }}{50}{table.caption.88}}
\newlabel{SMBTRMTRM5}{{5.8}{50}{TRRS results for $\Delta = 5 \%$\relax }{table.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces TRRS results for $\Delta = 10 \%$\relax }}{50}{table.caption.89}}
\newlabel{SMBTRMTRM10}{{5.9}{50}{TRRS results for $\Delta = 10 \%$\relax }{table.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparing \textbf  {TRM} with stochastic training example subsampling \textbf  {TRM} method, \textbf  {BTRM}, based on objective function over CPU time.\relax }}{52}{figure.caption.90}}
\newlabel{figure:BTRMTRM}{{5.5}{52}{Comparing \textbf {TRM} with stochastic training example subsampling \textbf {TRM} method, \textbf {BTRM}, based on objective function over CPU time.\relax }{figure.caption.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Reducing the Dimentionality of TRS}{53}{section.5.6}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces TRRS results for $\Delta = 1 \%$\relax }}{53}{table.caption.91}}
\newlabel{table:TRMWS1}{{5.10}{53}{TRRS results for $\Delta = 1 \%$\relax }{table.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces TRRS results for $\Delta = 5 \%$\relax }}{54}{table.caption.92}}
\newlabel{table:TRMWS5}{{5.11}{54}{TRRS results for $\Delta = 5 \%$\relax }{table.caption.92}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces TRRS results for $\Delta = 10 \%$\relax }}{54}{table.caption.93}}
\newlabel{table:TRMWS10}{{5.12}{54}{TRRS results for $\Delta = 10 \%$\relax }{table.caption.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Running TRM vs TRMWS for training ff-ANNs on five datasets.\relax }}{55}{figure.caption.94}}
\newlabel{figure:TRMWS}{{5.6}{55}{Running TRM vs TRMWS for training ff-ANNs on five datasets.\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Running BTRM vs BTRMWS for training ff-ANNs on five datasets.\relax }}{56}{figure.caption.95}}
\newlabel{figure:BTRMWS}{{5.7}{56}{Running BTRM vs BTRMWS for training ff-ANNs on five datasets.\relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Test results for using stochastic TRM methods.\relax }}{57}{figure.caption.96}}
\newlabel{figure:SMBTRMWS}{{5.8}{57}{Test results for using stochastic TRM methods.\relax }{figure.caption.96}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces TRRS results for $\Delta = 1\%$\relax }}{58}{table.caption.97}}
\newlabel{SMBTRMWS1}{{5.13}{58}{TRRS results for $\Delta = 1\%$\relax }{table.caption.97}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces TRRS results for $\Delta = 5\%$\relax }}{58}{table.caption.98}}
\newlabel{SMBTRMWS5}{{5.14}{58}{TRRS results for $\Delta = 5\%$\relax }{table.caption.98}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces TRRS results for $\Delta = 10\%$\relax }}{58}{table.caption.99}}
\newlabel{SMBTRMWS10}{{5.15}{58}{TRRS results for $\Delta = 10\%$\relax }{table.caption.99}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}CPU Time Analysis}{59}{section.5.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Percentage of time taken up by solving the Generalized Eigenvalue Problem (\ref  {equation:M_def}) and computing $\mathbf  {p}_{1}$ from the result at each step.\relax }}{60}{figure.caption.100}}
\newlabel{figure:p1_time}{{5.9}{60}{Percentage of time taken up by solving the Generalized Eigenvalue Problem (\ref {equation:M_def}) and computing $\mathbf {p}_{1}$ from the result at each step.\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Percentage of time taken for computing the Hessian matrix at each change in weight values $\mathbf  {w}$. Recall that only five of our methods include the computation of the Hessian matrix. \relax }}{60}{figure.caption.101}}
\newlabel{figure:H_time}{{5.10}{60}{Percentage of time taken for computing the Hessian matrix at each change in weight values $\mathbf {w}$. Recall that only five of our methods include the computation of the Hessian matrix. \relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Percentage of time taken to solve for the $\mathbf  {p}_{0}$, which is solving the linear equation (\ref  {equation:p0}). Recall that those methods which use the ATRRS do not compute $\mathbf  {p}_{0}$ (see Algorithm \ref  {algorithm:assrs-simple}).\relax }}{61}{figure.caption.102}}
\newlabel{figure:p0_time}{{5.11}{61}{Percentage of time taken to solve for the $\mathbf {p}_{0}$, which is solving the linear equation (\ref {equation:p0}). Recall that those methods which use the ATRRS do not compute $\mathbf {p}_{0}$ (see Algorithm \ref {algorithm:assrs-simple}).\relax }{figure.caption.102}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Convergence of \textbf  {TRM} with $\mathbf  {p}_{0}$ and $\mathbf  {p}_{1}$ using stopping criterion of residual magnitude less than $10^{-3}$ (dashed line) and stopping criterion of reaching the max iteration (\textbf  {submaxiter}=1, 2, 3, 4, 5). (Upper Left) Convergence in terms of CPU time. (Upper Right) Convergence in terms of Epoch. (Bottom) Convergence in terms of CPU time with \textbf  {SGD} for reference.\relax }}{62}{figure.caption.103}}
\newlabel{figure:mnist_maxiter}{{5.12}{62}{Convergence of \textbf {TRM} with $\mathbf {p}_{0}$ and $\mathbf {p}_{1}$ using stopping criterion of residual magnitude less than $10^{-3}$ (dashed line) and stopping criterion of reaching the max iteration (\textbf {submaxiter}=1, 2, 3, 4, 5). (Upper Left) Convergence in terms of CPU time. (Upper Right) Convergence in terms of Epoch. (Bottom) Convergence in terms of CPU time with \textbf {SGD} for reference.\relax }{figure.caption.103}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Robustness Test}{63}{section.5.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}The XOR Problem}{63}{subsection.5.8.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces The 2-feature XOR problem.\relax }}{63}{table.caption.104}}
\newlabel{table:XOR}{{5.16}{63}{The 2-feature XOR problem.\relax }{table.caption.104}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.17}{\ignorespaces First order critical point classification for the XOR problem when trained on a 2-layer ff-ANN with two hidden nodes.\relax }}{63}{table.caption.105}}
\newlabel{table:XOR_point_type}{{5.17}{63}{First order critical point classification for the XOR problem when trained on a 2-layer ff-ANN with two hidden nodes.\relax }{table.caption.105}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}XOR Training Results}{64}{subsection.5.8.2}}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\citation{Shepherd.1997}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Final points are recorded when the final objective function value is within a tolerance of $10^{-3}$ from one of the known points.\relax }}{65}{figure.caption.106}}
\newlabel{figure:XOR}{{5.13}{65}{Final points are recorded when the final objective function value is within a tolerance of $10^{-3}$ from one of the known points.\relax }{figure.caption.106}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Percentage of runs which successfully converge to the global minimum of the XOR problem. The methods from our exploration are displayed as solid black and methods from \cite  {Shepherd.1997} are coloured with a diagonal pattern.\relax }}{66}{figure.caption.107}}
\newlabel{figure:XOR_comparison}{{5.14}{66}{Percentage of runs which successfully converge to the global minimum of the XOR problem. The methods from our exploration are displayed as solid black and methods from \cite {Shepherd.1997} are coloured with a diagonal pattern.\relax }{figure.caption.107}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{67}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Future Work}{68}{section.6.1}}
\@writefile{toc}{\contentsline {chapter}{APPENDICES}{70}{appendix*.108}}
\citation{Pearlmutter.1993}
\citation{Pearlmutter.1993}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Pearlmutter Trick for Computing Second Order Information}{71}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:pearlmutter}{{A}{71}{Pearlmutter Trick for Computing Second Order Information}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.0.1}Forward and Back Propagations}{72}{subsection.A.0.1}}
\newlabel{appendix:forandback}{{A.0.1}{72}{Forward and Back Propagations}{subsection.A.0.1}{}}
\newlabel{equation:map}{{A.3}{72}{Forward and Back Propagations}{equation.A.0.3}{}}
\newlabel{equation:index}{{A.4}{72}{Forward and Back Propagations}{equation.A.0.4}{}}
\newlabel{equation:biasindex}{{A.5}{72}{Forward and Back Propagations}{AMS.113}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Values of parameters for Algorithm \ref  {algorithm:passes} based on propagation direction.\relax }}{73}{table.caption.115}}
\newlabel{table:passes}{{A.1}{73}{Values of parameters for Algorithm \ref {algorithm:passes} based on propagation direction.\relax }{table.caption.115}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Pseudocode of a Single Pass\relax }}{73}{algorithm.11}}
\newlabel{euclid}{{11}{73}{Pseudocode of a Single Pass\relax }{algorithm.11}{}}
\newlabel{algorithm:passes}{{11}{73}{Pseudocode of a Single Pass\relax }{algorithm.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Forward Propagation I}{74}{section*.116}}
\newlabel{equation:pass1_pe}{{A.7}{74}{Forward Propagation I}{equation.A.0.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Back Propagation I}{74}{section*.119}}
\newlabel{equation:df_dzL0}{{A.9}{74}{Back Propagation I}{equation.A.0.9}{}}
\newlabel{equation:pass2_pe}{{A.10}{75}{Back Propagation I}{equation.A.0.10}{}}
\newlabel{equation:grad_w}{{A.11}{75}{Back Propagation I}{equation.A.0.11}{}}
\newlabel{equation:grad_b}{{A.12}{75}{Back Propagation I}{equation.A.0.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Forward Propagation II}{75}{section*.125}}
\newlabel{equation:pass3_pe}{{A.15}{75}{Forward Propagation II}{equation.A.0.15}{}}
\newlabel{equation:RwisV}{{A.17}{76}{Forward Propagation II}{equation.A.0.17}{}}
\newlabel{equation:WtoV}{{A.18}{76}{Forward Propagation II}{equation.A.0.18}{}}
\newlabel{equation:btobv}{{A.19}{76}{Forward Propagation II}{equation.A.0.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Back Propagation II}{76}{section*.132}}
\newlabel{equation:rh}{{A.23}{76}{Back Propagation II}{equation.A.0.23}{}}
\newlabel{equation:rdh}{{A.25}{77}{Back Propagation II}{equation.A.0.25}{}}
\newlabel{equation:pass4_pe}{{A.26}{77}{Back Propagation II}{equation.A.0.26}{}}
\newlabel{equation:hv_w}{{A.27}{77}{Back Propagation II}{equation.A.0.27}{}}
\newlabel{equation:hv_b}{{A.28}{77}{Back Propagation II}{equation.A.0.28}{}}
\newlabel{equation:biasindex}{{A.29}{78}{Back Propagation II}{AMS.142}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Complexity Analysis}{78}{section.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Complexity Analysis of The Pearlmutter Trick for ff-ANNs}{78}{subsection.A.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Forward Propagation I}{78}{section*.143}}
\@writefile{toc}{\contentsline {subsubsection}{Back Propagation I}{79}{section*.146}}
\@writefile{toc}{\contentsline {subsubsection}{Forward Propagation II}{79}{section*.149}}
\@writefile{toc}{\contentsline {subsubsection}{Back Propagation II}{80}{section*.151}}
\@writefile{toc}{\contentsline {subsubsection}{Softmax vs Sigmoid}{80}{section*.153}}
\newlabel{equation:simplifieda}{{A.42}{82}{Softmax vs Sigmoid}{equation.A.1.42}{}}
\newlabel{equation:simplifiedb}{{A.43}{82}{Softmax vs Sigmoid}{equation.A.1.43}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Hyperparameters}{84}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:hyperparameters}{{B}{84}{Hyperparameters}{appendix.B}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces Hyperparameter Protocol\relax }}{85}{algorithm.12}}
\newlabel{algorithm:hyperparameters}{{12}{85}{Hyperparameter Protocol\relax }{algorithm.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces  The \textbf  {Method} is the algorithm used to test the \textbf  {hyperparameter} in the same row. This mapping is used in Algorithm \ref  {algorithm:hyperparameters}, row 3.\relax }}{85}{table.caption.167}}
\newlabel{table:hyperparameters}{{B.1}{85}{The \textbf {Method} is the algorithm used to test the \textbf {hyperparameter} in the same row. This mapping is used in Algorithm \ref {algorithm:hyperparameters}, row 3.\relax }{table.caption.167}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Full Hessian vs Hessian Free}{86}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:fhhf}{{C}{86}{Full Hessian vs Hessian Free}{appendix.C}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Time taken to compute min(1000,max iterations) using the full H approach and the HF approach.\relax }}{86}{table.caption.168}}
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces Time taken to compute min(1000,max iterations) iterations using the full H approach and the HF approach.\relax }}{87}{table.caption.169}}
\bibstyle{plain}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Hyperparameter Test Results}{88}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:hyperparameter_values}{{D}{88}{Hyperparameter Test Results}{appendix.D}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D.1}{\ignorespaces Final objective function achieved for training an ff-ANN on the set of datasets using SGD and using TRM.\relax }}{88}{table.caption.170}}
\newlabel{table:hyperparam}{{D.1}{88}{Final objective function achieved for training an ff-ANN on the set of datasets using SGD and using TRM.\relax }{table.caption.170}{}}
\bibdata{uw-ethesis}
\bibcite{neural1}{1}
\bibcite{tutorial}{2}
\bibcite{TRM.book}{3}
\bibcite{bojarski2016end}{4}
\bibcite{boyd}{5}
\bibcite{saddle_free}{6}
\bibcite{demmel.book}{7}
\bibcite{Nurs}{8}
\bibcite{IRIS}{9}
\bibcite{Fletcher.1987}{10}
\bibcite{golub1989matrix}{11}
\@writefile{toc}{\contentsline {chapter}{\textbf  {References}}{89}{section*.171}}
\bibcite{Goodfellow-et-al-2016}{12}
\bibcite{neural2}{13}
\bibcite{gulcehre2017robust}{14}
\bibcite{Derm}{15}
\bibcite{Habe}{16}
\bibcite{hastie2009overview}{17}
\bibcite{hornik1989multilayer}{18}
\bibcite{gd_converges.paper}{19}
\bibcite{krizhevsky2012imagenet}{20}
\bibcite{mnist}{21}
\bibcite{ml_repo}{22}
\bibcite{ma2002natural}{23}
\bibcite{mahsereci2017early}{24}
\bibcite{martens2010deep}{25}
\bibcite{Perceptrons.book}{26}
\bibcite{Pearlmutter.1993}{27}
\bibcite{priddy2005artificial}{28}
\bibcite{adachi.paper}{29}
\bibcite{Shepherd.1997}{30}
\bibcite{shewchuk1994introduction}{31}
\bibcite{shultz1985family}{32}
\bibcite{silver2016mastering}{33}
\bibcite{simpson2015oddball}{34}
\bibcite{dropout}{35}
\bibcite{strakovs1991real}{36}
\bibcite{tetko1995neural}{37}
\bibcite{vapnik1999overview}{38}
\bibcite{vapnik2015uniform}{39}
\bibcite{wright1999numerical}{40}
\bibcite{IRAM}{41}
\citation{*}
